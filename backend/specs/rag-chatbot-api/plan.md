# Implementation Plan: RAG Chatbot API

**Branch**: `main` | **Date**: 2025-12-18 | **Spec**: [spec.md](./spec.md)
**Input**: Feature specification from `specs/rag-chatbot-api/spec.md`

## Summary

Build a FastAPI backend that provides RAG question-answering for Physical AI textbook using Gemini, Cohere, and Qdrant. System must answer questions exclusively from textbook content with zero hallucination, respond in <3 seconds, ingest content in <5 minutes, and operate entirely on free tiers. Uses OpenAI Agents SDK pattern with Gemini 2.0 Flash Exp via OpenAI-compatible endpoint, Cohere embed-english-v3.0 for embeddings, and Qdrant Cloud for vector storage.

## Technical Context

**Language/Version**: Python 3.11+
**Primary Dependencies**: FastAPI, Uvicorn, Qdrant Client, Cohere SDK, OpenAI SDK, Trafilatura, Pydantic
**Storage**: Qdrant Cloud (vector database) - no local persistence
**Testing**: pytest, pytest-asyncio, httpx (FastAPI test client)
**Target Platform**: Vercel (primary), Railway (alternative), any Python 3.11+ ASGI host
**Project Type**: Single backend API (stateless)
**Performance Goals**:
- POST /chat: p95 < 3s
- POST /ingest: < 5 minutes total
- GET /health: p95 < 100ms
- Memory: < 512MB under load

**Constraints**:
- Free tier only (Qdrant, Cohere, Gemini)
- Zero hallucination tolerance
- Stateless (no session/history persistence)
- CORS restricted to single frontend origin
- No authentication (MVP)

**Scale/Scope**:
- ~50 textbook pages
- ~1000 chunks after ingestion
- <1000 queries/day expected
- Single collection, single tenant

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

✅ **Content Fidelity**: Agent system prompt enforces textbook-only responses, retrieve() tool provides only textbook chunks, explicit "I don't know" fallback
✅ **Speed & Performance**: Async I/O throughout, Cohere batch embeddings, connection pooling for Qdrant, performance monitoring
✅ **Simplicity First**: No auth, no history, no caching, direct API→Service→Client pattern, <500 lines per file
✅ **Free Tier Operations**: All services use free tier, no paid dependencies
✅ **Security & Configuration**: All secrets in .env, input validation on all endpoints, CORS whitelist, structured logging
✅ **API Design**: RESTful endpoints, Pydantic models, OpenAPI auto-generation, proper HTTP status codes

**No constitutional violations** - all design decisions align with principles.

## Project Structure

### Documentation (this feature)

```text
specs/rag-chatbot-api/
├── spec.md                 # Feature specification (completed)
├── plan.md                 # This file
├── data-model.md           # Phase 1: Pydantic models and data contracts
├── api-contracts.md        # Phase 1: API endpoint contracts with examples
├── agent-design.md         # Phase 1: Gemini agent configuration and tool definitions
└── tasks.md                # Phase 2: Generated by /sp.tasks (NOT created by /sp.plan)
```

### Source Code (repository root)

```text
backend/                    # Root directory
├── src/
│   ├── main.py            # FastAPI app initialization, CORS, middleware
│   ├── config.py          # Environment variable loading with Pydantic BaseSettings
│   ├── logger.py          # Structured logging configuration
│   │
│   ├── models/            # Pydantic data models
│   │   ├── __init__.py
│   │   ├── chat.py        # ChatRequest, ChatResponse models
│   │   ├── ingest.py      # IngestRequest, IngestResponse models
│   │   └── health.py      # HealthResponse model
│   │
│   ├── services/          # Business logic layer
│   │   ├── __init__.py
│   │   ├── embedding.py   # Cohere embedding service
│   │   ├── vector_store.py # Qdrant client wrapper
│   │   ├── agent.py       # Gemini agent with retrieve() tool
│   │   ├── ingestion.py   # Sitemap fetching, text extraction, chunking
│   │   └── retrieval.py   # Query embedding + Qdrant search
│   │
│   ├── api/               # FastAPI route handlers
│   │   ├── __init__.py
│   │   ├── chat.py        # POST /chat endpoint
│   │   ├── ingest.py      # POST /ingest endpoint
│   │   └── health.py      # GET /health endpoint
│   │
│   └── utils/             # Shared utilities
│       ├── __init__.py
│       ├── chunking.py    # Text chunking with overlap
│       ├── validation.py  # Input validation helpers
│       └── exceptions.py  # Custom exception classes
│
├── tests/
│   ├── __init__.py
│   ├── conftest.py        # Pytest fixtures and mocks
│   │
│   ├── unit/              # Unit tests (services, utils)
│   │   ├── test_embedding.py
│   │   ├── test_chunking.py
│   │   ├── test_agent.py
│   │   └── test_validation.py
│   │
│   └── integration/       # API endpoint tests
│       ├── test_chat.py
│       ├── test_ingest.py
│       └── test_health.py
│
├── .env.example           # Example environment variables
├── requirements.txt       # Python dependencies
├── vercel.json           # Vercel deployment configuration
└── README.md             # Setup and usage instructions
```

**Structure Decision**: Single backend API structure selected because:
1. Project is backend-only (frontend exists separately)
2. Stateless design requires no database migrations or persistence layer
3. Service layer pattern provides clear separation of concerns
4. All files under 500 lines per constitution
5. Simple enough for fast deployment (<10 minutes)

## Complexity Tracking

No constitutional violations - table not needed.

## Architecture Design

### 1. System Components

#### 1.1 FastAPI Application (`src/main.py`)
**Responsibility**: Application initialization, CORS middleware, request/response lifecycle

**Key Features**:
- CORS middleware configured for FRONTEND_URL only
- Request ID middleware for tracing
- Structured logging middleware
- Exception handlers for custom exceptions
- Health check liveliness
- OpenAPI documentation at /docs

**Dependencies**: FastAPI, Uvicorn, middleware libraries

**Configuration**:
```python
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from src.config import settings

app = FastAPI(
    title="RAG Chatbot API",
    version="1.0.0",
    description="Physical AI Textbook Question Answering"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=[settings.FRONTEND_URL],
    allow_credentials=True,
    allow_methods=["GET", "POST"],
    allow_headers=["*"],
)
```

#### 1.2 Configuration Service (`src/config.py`)
**Responsibility**: Load and validate environment variables

**Key Features**:
- Pydantic BaseSettings for validation
- Type-safe configuration access
- Default values where appropriate
- Fail-fast on missing required vars

**Environment Variables**:
```python
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    # API Keys
    GEMINI_API_KEY: str
    COHERE_API_KEY: str
    QDRANT_API_KEY: str

    # Service URLs
    QDRANT_URL: str
    FRONTEND_URL: str
    SITEMAP_URL: str = "https://ai-native-text-book.vercel.app/sitemap.xml"

    # Collection Config
    QDRANT_COLLECTION: str = "ai-textbook"

    # Chunking Config
    CHUNK_SIZE: int = 1000
    CHUNK_OVERLAP: int = 200

    # Performance Config
    MAX_QUERY_LENGTH: int = 500
    TOP_K_RESULTS: int = 5
    EMBEDDING_BATCH_SIZE: int = 96  # Cohere free tier limit

    class Config:
        env_file = ".env"
        case_sensitive = True

settings = Settings()
```

#### 1.3 Embedding Service (`src/services/embedding.py`)
**Responsibility**: Generate embeddings using Cohere API

**Key Features**:
- Cohere embed-english-v3.0 model
- Batch processing for ingestion
- Exponential backoff on rate limits
- 1024-dimensional vectors

**Interface**:
```python
class EmbeddingService:
    async def embed_text(self, text: str) -> List[float]
    async def embed_batch(self, texts: List[str]) -> List[List[float]]
```

**Implementation Notes**:
- Use cohere.AsyncClient for async operations
- Handle rate limits with retry logic (3 attempts, exponential backoff)
- Log embedding latency for monitoring
- Validate output dimensions (must be 1024)

#### 1.4 Vector Store Service (`src/services/vector_store.py`)
**Responsibility**: Interact with Qdrant Cloud

**Key Features**:
- Collection management (create if not exists)
- Upsert chunks with metadata
- Semantic search with score threshold
- Connection pooling
- Health checks

**Interface**:
```python
class VectorStoreService:
    async def ensure_collection_exists(self) -> None
    async def upsert_chunks(self, chunks: List[TextChunk]) -> int
    async def search(self, query_vector: List[float], top_k: int = 5) -> List[SearchResult]
    async def get_collection_count(self) -> int
    async def health_check(self) -> bool
```

**Data Model** (Qdrant point structure):
```python
{
    "id": str(uuid),
    "vector": List[float],  # 1024-dim
    "payload": {
        "text": str,        # Chunk content
        "url": str,         # Source page URL
        "title": str,       # Page title
        "chunk_index": int  # Position in page
    }
}
```

#### 1.5 Agent Service (`src/services/agent.py`)
**Responsibility**: Gemini agent with retrieve() tool

**Key Features**:
- OpenAI SDK configured for Gemini endpoint
- System prompt enforcing textbook-only responses
- retrieve() tool for Qdrant search
- Source URL extraction from responses

**Configuration**:
```python
from openai import AsyncOpenAI

class AgentService:
    def __init__(self):
        self.client = AsyncOpenAI(
            api_key=settings.GEMINI_API_KEY,
            base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
        )
        self.model = "gemini-1.5-flash"
        self.system_prompt = """You are an AI tutor for a Physical AI textbook.

CRITICAL RULES:
1. Use the retrieve() tool to search for relevant content
2. Answer questions ONLY using information from the retrieved textbook chunks
3. If the information is not in the textbook, respond exactly: "I don't have information about this in the textbook"
4. Always cite your sources by mentioning which sections you used
5. Do not use external knowledge or make assumptions beyond what's in the textbook"""

    async def answer_question(self, query: str) -> Tuple[str, List[str]]
```

**Tool Definition**:
```python
{
    "type": "function",
    "function": {
        "name": "retrieve",
        "description": "Search the Physical AI textbook for relevant content. Use this tool before answering any question.",
        "parameters": {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "The semantic search query to find relevant textbook content"
                }
            },
            "required": ["query"]
        }
    }
}
```

#### 1.6 Ingestion Service (`src/services/ingestion.py`)
**Responsibility**: Fetch, extract, chunk, embed, and store textbook content

**Key Features**:
- Fetch sitemap.xml and parse URLs
- Extract text with Trafilatura
- Chunk with overlap using utility
- Batch embed with Cohere
- Upsert to Qdrant
- Progress logging every 10 pages
- In-memory lock to prevent duplicate ingestion

**Interface**:
```python
class IngestionService:
    async def ingest_from_sitemap(self, sitemap_url: str) -> IngestionResult
```

**Flow**:
```python
1. Check if ingestion already running (in-memory flag)
2. Fetch sitemap.xml
3. Parse XML to extract <url> elements
4. For each URL:
   a. Fetch HTML with httpx
   b. Extract text with trafilatura.extract()
   c. Chunk text with overlap (1000 chars, 200 overlap)
   d. Add to batch buffer
   e. When batch reaches EMBEDDING_BATCH_SIZE:
      - Embed batch with Cohere
      - Upsert to Qdrant
      - Clear buffer
5. Process remaining buffer
6. Return IngestionResult with counts and duration
```

#### 1.7 Retrieval Service (`src/services/retrieval.py`)
**Responsibility**: Combine embedding + vector search

**Key Features**:
- Embed user query
- Search Qdrant for top-k chunks
- Extract unique source URLs
- Return formatted results

**Interface**:
```python
class RetrievalService:
    async def retrieve(self, query: str, top_k: int = 5) -> RetrievalResult

@dataclass
class RetrievalResult:
    chunks: List[TextChunk]
    sources: List[str]  # Unique URLs
    scores: List[float]
```

#### 1.8 Chunking Utility (`src/utils/chunking.py`)
**Responsibility**: Split text into overlapping chunks

**Key Features**:
- Fixed-size chunks (1000 chars default)
- Overlap for context continuity (200 chars default)
- Respect word boundaries where possible
- Handle edge cases (text shorter than chunk size)

**Interface**:
```python
def chunk_text(
    text: str,
    chunk_size: int = 1000,
    overlap: int = 200
) -> List[str]
```

**Algorithm**:
```python
1. If len(text) <= chunk_size, return [text]
2. Start at position 0
3. While position < len(text):
   a. Extract chunk[position:position+chunk_size]
   b. Find last space in chunk to avoid word splits
   c. Append chunk to results
   d. Move position forward by (chunk_size - overlap)
4. Return chunks
```

### 2. API Endpoints

#### 2.1 POST /chat (`src/api/chat.py`)

**Request Flow**:
```
1. Validate input (Pydantic model)
2. Generate request_id (uuid4)
3. Log request start
4. Call retrieval_service.retrieve(query)
5. Call agent_service.answer_question(query, chunks)
6. Calculate response_time
7. Log request completion
8. Return ChatResponse
```

**Request Model**:
```python
class ChatRequest(BaseModel):
    query: str = Field(..., min_length=1, max_length=500)

    @validator('query')
    def query_not_empty(cls, v):
        if not v.strip():
            raise ValueError("Question cannot be empty")
        return v.strip()
```

**Response Model**:
```python
class ChatResponse(BaseModel):
    answer: str
    sources: List[str]
    response_time: float  # seconds
    request_id: str
```

**Error Handling**:
- 400: Empty query, too long query, invalid JSON
- 503: Cohere API failure, Gemini API failure, Qdrant unavailable
- 504: Qdrant timeout (>5s)
- 500: Unexpected errors

**Implementation**:
```python
@router.post("/chat", response_model=ChatResponse)
async def chat(
    request: ChatRequest,
    retrieval_service: RetrievalService = Depends(),
    agent_service: AgentService = Depends()
) -> ChatResponse:
    request_id = str(uuid4())
    start_time = time.time()

    try:
        logger.info(f"Chat request", extra={
            "request_id": request_id,
            "query_length": len(request.query)
        })

        # Retrieve relevant chunks
        retrieval_result = await retrieval_service.retrieve(request.query)

        # Generate answer with agent
        answer, sources = await agent_service.answer_question(
            request.query,
            retrieval_result.chunks
        )

        response_time = time.time() - start_time

        logger.info(f"Chat response", extra={
            "request_id": request_id,
            "response_time": response_time,
            "sources_count": len(sources)
        })

        return ChatResponse(
            answer=answer,
            sources=sources,
            response_time=response_time,
            request_id=request_id
        )

    except CohereAPIError as e:
        logger.error(f"Cohere API error", extra={"request_id": request_id, "error": str(e)})
        raise HTTPException(status_code=503, detail="AI service temporarily unavailable")
    except GeminiAPIError as e:
        logger.error(f"Gemini API error", extra={"request_id": request_id, "error": str(e)})
        raise HTTPException(status_code=503, detail="AI service temporarily unavailable")
    except QdrantTimeout as e:
        logger.error(f"Qdrant timeout", extra={"request_id": request_id})
        raise HTTPException(status_code=504, detail="Search service timeout")
```

#### 2.2 POST /ingest (`src/api/ingest.py`)

**Request Flow**:
```
1. Validate input (optional sitemap_url)
2. Check if ingestion already running (409 if yes)
3. Set in-memory lock
4. Call ingestion_service.ingest_from_sitemap()
5. Release lock
6. Return IngestResponse
```

**Request Model**:
```python
class IngestRequest(BaseModel):
    sitemap_url: Optional[str] = None  # Defaults to config.SITEMAP_URL
```

**Response Model**:
```python
class IngestResponse(BaseModel):
    status: str  # "success" | "partial_success" | "failed"
    pages_processed: int
    chunks_created: int
    duration_seconds: float
    errors: Optional[List[str]] = None  # Page URLs that failed
```

**Error Handling**:
- 400: Invalid sitemap URL, malformed sitemap XML
- 409: Ingestion already in progress
- 503: Cohere API failure preventing completion
- 500: Unexpected errors

**Implementation**:
```python
ingestion_lock = asyncio.Lock()

@router.post("/ingest", response_model=IngestResponse)
async def ingest(
    request: IngestRequest = Body(default=IngestRequest()),
    ingestion_service: IngestionService = Depends()
) -> IngestResponse:
    if ingestion_lock.locked():
        raise HTTPException(status_code=409, detail="Ingestion already in progress")

    async with ingestion_lock:
        sitemap_url = request.sitemap_url or settings.SITEMAP_URL

        logger.info(f"Starting ingestion", extra={"sitemap_url": sitemap_url})

        try:
            result = await ingestion_service.ingest_from_sitemap(sitemap_url)

            logger.info(f"Ingestion completed", extra={
                "pages_processed": result.pages_processed,
                "chunks_created": result.chunks_created,
                "duration": result.duration_seconds
            })

            return IngestResponse(
                status="success" if not result.errors else "partial_success",
                pages_processed=result.pages_processed,
                chunks_created=result.chunks_created,
                duration_seconds=result.duration_seconds,
                errors=result.errors
            )

        except SitemapFetchError as e:
            logger.error(f"Sitemap fetch failed", extra={"error": str(e)})
            raise HTTPException(status_code=400, detail="Cannot fetch or parse sitemap")
```

#### 2.3 GET /health (`src/api/health.py`)

**Request Flow**:
```
1. Check Qdrant connectivity
2. Check collection exists
3. Get collection count
4. Return HealthResponse
```

**Response Model**:
```python
class HealthResponse(BaseModel):
    status: str  # "healthy" | "unhealthy"
    qdrant_status: str  # "connected" | "unreachable"
    collection_exists: bool
    collection_count: Optional[int] = None
    timestamp: str  # ISO8601
```

**Error Handling**:
- 503: Qdrant unreachable or connection timeout
- 200: Always return 200 with status field indicating health

**Implementation**:
```python
@router.get("/health", response_model=HealthResponse)
async def health(
    vector_store: VectorStoreService = Depends()
) -> HealthResponse:
    try:
        is_healthy = await vector_store.health_check()
        collection_count = await vector_store.get_collection_count() if is_healthy else None

        return HealthResponse(
            status="healthy" if is_healthy else "unhealthy",
            qdrant_status="connected" if is_healthy else "unreachable",
            collection_exists=is_healthy,
            collection_count=collection_count,
            timestamp=datetime.utcnow().isoformat() + "Z"
        )

    except Exception as e:
        logger.error(f"Health check failed", extra={"error": str(e)})
        return HealthResponse(
            status="unhealthy",
            qdrant_status="unreachable",
            collection_exists=False,
            timestamp=datetime.utcnow().isoformat() + "Z"
        )
```

### 3. Data Models

**Location**: `src/models/`

#### 3.1 Chat Models (`src/models/chat.py`)
```python
from pydantic import BaseModel, Field, validator
from typing import List

class ChatRequest(BaseModel):
    query: str = Field(..., min_length=1, max_length=500, description="User question")

    @validator('query')
    def query_not_empty(cls, v):
        if not v.strip():
            raise ValueError("Question cannot be empty")
        return v.strip()

class ChatResponse(BaseModel):
    answer: str = Field(..., description="AI-generated answer")
    sources: List[str] = Field(..., description="Source URLs from textbook")
    response_time: float = Field(..., description="Response time in seconds")
    request_id: str = Field(..., description="Request trace ID")
```

#### 3.2 Ingest Models (`src/models/ingest.py`)
```python
from pydantic import BaseModel, Field, HttpUrl
from typing import Optional, List

class IngestRequest(BaseModel):
    sitemap_url: Optional[HttpUrl] = Field(None, description="Override default sitemap URL")

class IngestResponse(BaseModel):
    status: str = Field(..., description="success | partial_success | failed")
    pages_processed: int = Field(..., description="Number of pages processed")
    chunks_created: int = Field(..., description="Number of chunks created")
    duration_seconds: float = Field(..., description="Total ingestion time")
    errors: Optional[List[str]] = Field(None, description="URLs that failed to process")
```

#### 3.3 Health Models (`src/models/health.py`)
```python
from pydantic import BaseModel, Field
from typing import Optional

class HealthResponse(BaseModel):
    status: str = Field(..., description="healthy | unhealthy")
    qdrant_status: str = Field(..., description="connected | unreachable")
    collection_exists: bool = Field(..., description="Whether collection exists")
    collection_count: Optional[int] = Field(None, description="Number of chunks in collection")
    timestamp: str = Field(..., description="ISO8601 timestamp")
```

#### 3.4 Internal Models (`src/models/chunk.py`)
```python
from dataclasses import dataclass
from typing import List
from uuid import UUID

@dataclass
class TextChunk:
    id: UUID
    text: str
    url: str
    title: str
    chunk_index: int
    vector: List[float]  # 1024-dim

@dataclass
class SearchResult:
    chunk: TextChunk
    score: float

@dataclass
class RetrievalResult:
    chunks: List[TextChunk]
    sources: List[str]
    scores: List[float]

@dataclass
class IngestionResult:
    pages_processed: int
    chunks_created: int
    duration_seconds: float
    errors: List[str]
```

### 4. Error Handling Strategy

**Custom Exceptions** (`src/utils/exceptions.py`):
```python
class RAGChatbotException(Exception):
    """Base exception"""
    pass

class CohereAPIError(RAGChatbotException):
    """Cohere API failures"""
    pass

class GeminiAPIError(RAGChatbotException):
    """Gemini API failures"""
    pass

class QdrantError(RAGChatbotException):
    """Qdrant connection/query failures"""
    pass

class QdrantTimeout(QdrantError):
    """Qdrant query timeout (>5s)"""
    pass

class SitemapFetchError(RAGChatbotException):
    """Sitemap fetch/parse failures"""
    pass

class ValidationError(RAGChatbotException):
    """Input validation failures"""
    pass
```

**Exception Handler** (`src/main.py`):
```python
@app.exception_handler(RAGChatbotException)
async def ragchatbot_exception_handler(request: Request, exc: RAGChatbotException):
    logger.error(f"Application error", extra={
        "path": request.url.path,
        "error": str(exc),
        "type": exc.__class__.__name__
    })

    status_code = 500
    if isinstance(exc, ValidationError):
        status_code = 400
    elif isinstance(exc, (CohereAPIError, GeminiAPIError, QdrantError)):
        status_code = 503
    elif isinstance(exc, QdrantTimeout):
        status_code = 504
    elif isinstance(exc, SitemapFetchError):
        status_code = 400

    return JSONResponse(
        status_code=status_code,
        content={
            "detail": str(exc),
            "request_id": request.state.request_id if hasattr(request.state, 'request_id') else None,
            "timestamp": datetime.utcnow().isoformat() + "Z"
        }
    )
```

### 5. Logging Strategy

**Logger Configuration** (`src/logger.py`):
```python
import logging
import json
from datetime import datetime

class StructuredLogger:
    def __init__(self, name: str):
        self.logger = logging.getLogger(name)
        handler = logging.StreamHandler()
        handler.setFormatter(StructuredFormatter())
        self.logger.addHandler(handler)
        self.logger.setLevel(logging.INFO)

    def info(self, message: str, **kwargs):
        self.logger.info(message, extra=kwargs)

    def error(self, message: str, **kwargs):
        self.logger.error(message, extra=kwargs)

class StructuredFormatter(logging.Formatter):
    def format(self, record):
        log_data = {
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "level": record.levelname,
            "message": record.getMessage(),
            "module": record.module,
        }

        # Add extra fields
        if hasattr(record, '__dict__'):
            for key, value in record.__dict__.items():
                if key not in ['name', 'msg', 'args', 'created', 'filename', 'funcName',
                              'levelname', 'levelno', 'lineno', 'module', 'msecs',
                              'pathname', 'process', 'processName', 'relativeCreated',
                              'thread', 'threadName']:
                    log_data[key] = value

        return json.dumps(log_data)

logger = StructuredLogger("ragchatbot")
```

**Required Log Fields** (per constitution):
- timestamp (ISO8601)
- level (INFO|WARN|ERROR)
- message (string)
- request_id (uuid)
- endpoint (string)
- duration_ms (float)

### 6. Testing Strategy

#### 6.1 Unit Tests

**Test Coverage**:
- `test_embedding.py`: Mock Cohere API, test batch processing, error handling
- `test_chunking.py`: Test various text sizes, overlap logic, word boundaries
- `test_agent.py`: Mock OpenAI client, test retrieve() tool, response parsing
- `test_validation.py`: Test input validation edge cases

**Fixtures** (`conftest.py`):
```python
import pytest
from unittest.mock import AsyncMock, MagicMock

@pytest.fixture
def mock_cohere_client():
    client = AsyncMock()
    client.embed.return_value = MagicMock(embeddings=[[0.1] * 1024])
    return client

@pytest.fixture
def mock_qdrant_client():
    client = AsyncMock()
    client.search.return_value = []
    client.upsert.return_value = MagicMock(status="completed")
    return client

@pytest.fixture
def mock_openai_client():
    client = AsyncMock()
    # Mock agent response
    return client
```

#### 6.2 Integration Tests

**Test Coverage**:
- `test_chat.py`: Full POST /chat flow with mocked external APIs
- `test_ingest.py`: Full POST /ingest flow with mocked sitemap and APIs
- `test_health.py`: GET /health with various Qdrant states

**Test Client**:
```python
from fastapi.testclient import TestClient
from src.main import app

@pytest.fixture
def client():
    return TestClient(app)

def test_chat_success(client, mock_services):
    response = client.post("/chat", json={"query": "What is AI?"})
    assert response.status_code == 200
    data = response.json()
    assert "answer" in data
    assert "sources" in data
    assert data["response_time"] < 3.0
```

#### 6.3 Test Requirements

- Test coverage > 80% (constitution requirement)
- Mock all external APIs (Cohere, Gemini, Qdrant)
- Test error paths explicitly
- Test edge cases from spec (empty query, long query, no results, etc.)
- Integration tests for each API endpoint
- Performance assertions (response times)

### 7. Deployment Configuration

#### 7.1 Vercel Configuration (`vercel.json`)
```json
{
  "version": 2,
  "builds": [
    {
      "src": "src/main.py",
      "use": "@vercel/python"
    }
  ],
  "routes": [
    {
      "src": "/(.*)",
      "dest": "src/main.py"
    }
  ],
  "env": {
    "GEMINI_API_KEY": "@gemini-api-key",
    "COHERE_API_KEY": "@cohere-api-key",
    "QDRANT_API_KEY": "@qdrant-api-key",
    "QDRANT_URL": "@qdrant-url",
    "QDRANT_COLLECTION": "ai-textbook",
    "FRONTEND_URL": "https://ai-native-text-book.vercel.app",
    "SITEMAP_URL": "https://ai-native-text-book.vercel.app/sitemap.xml"
  }
}
```

#### 7.2 Environment Variables (`.env.example`)
```bash
# API Keys
GEMINI_API_KEY=your_gemini_api_key_here
COHERE_API_KEY=your_cohere_api_key_here
QDRANT_API_KEY=your_qdrant_api_key_here

# Service URLs
QDRANT_URL=https://your-qdrant-instance.cloud.qdrant.io
FRONTEND_URL=https://ai-native-text-book.vercel.app
SITEMAP_URL=https://ai-native-text-book.vercel.app/sitemap.xml

# Collection Config
QDRANT_COLLECTION=ai-textbook

# Chunking Config
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# Performance Config
MAX_QUERY_LENGTH=500
TOP_K_RESULTS=5
EMBEDDING_BATCH_SIZE=96
```

#### 7.3 Dependencies (`requirements.txt`)
```
fastapi==0.109.0
uvicorn[standard]==0.27.0
pydantic==2.5.3
pydantic-settings==2.1.0
python-dotenv==1.0.0

# External APIs
qdrant-client==1.7.0
cohere==4.40
openai==1.10.0

# Text Processing
trafilatura==1.6.3

# Testing
pytest==7.4.4
pytest-asyncio==0.23.3
httpx==0.26.0
pytest-cov==4.1.0
```

### 8. Key Decisions

#### Decision 1: OpenAI SDK for Gemini
**Options Considered**:
1. Google's native Gemini SDK
2. OpenAI SDK with Gemini OpenAI-compatible endpoint
3. Direct HTTP requests to Gemini API

**Decision**: Option 2 - OpenAI SDK with Gemini endpoint

**Rationale**:
- OpenAI Agents SDK pattern provides clean tool/function calling
- Gemini's OpenAI-compatible endpoint supports this pattern
- Simpler agent implementation than native SDK
- Reduces learning curve (OpenAI SDK well-documented)
- Future-proof if switching LLM providers

**Trade-offs**:
- Slightly less control than native SDK
- Depends on Gemini maintaining OpenAI compatibility
- May miss Gemini-specific features

#### Decision 2: In-Memory Ingestion Lock
**Options Considered**:
1. Redis lock for distributed systems
2. Database lock with persistence
3. In-memory asyncio.Lock

**Decision**: Option 3 - In-memory asyncio.Lock

**Rationale**:
- Simplicity First principle (constitution)
- Single instance deployment (no horizontal scaling in MVP)
- No additional infrastructure (free tier constraint)
- Ingestion is rare operation (not high concurrency)
- Fast to implement

**Trade-offs**:
- Won't work with multiple API instances
- Lock state lost on restart
- Fine for MVP, may need Redis in future

#### Decision 3: No Caching Layer
**Options Considered**:
1. Redis cache for query results
2. In-memory LRU cache
3. No caching

**Decision**: Option 3 - No caching

**Rationale**:
- Simplicity First principle (constitution explicitly says "no caching in MVP")
- Adds complexity for uncertain benefit
- Query patterns unknown (may not have cache hits)
- Free tier constraint (Redis adds cost or complexity)
- Premature optimization

**Trade-offs**:
- Every query hits Cohere + Qdrant + Gemini
- Slower for repeated queries
- Higher API usage
- Can add later if needed

#### Decision 4: Stateless API
**Options Considered**:
1. Store chat history in database
2. Store conversation context in sessions
3. Fully stateless (no persistence)

**Decision**: Option 3 - Fully stateless

**Rationale**:
- Constitution explicitly forbids chat history (MVP phase)
- Reduces complexity (no database setup/migrations)
- Faster deployment
- Each query independent (easier to scale)
- Aligns with YAGNI principle

**Trade-offs**:
- No conversation context (each query isolated)
- No user personalization
- No usage tracking per user
- Fine for MVP, can add later

#### Decision 5: 1000-char chunks with 200-char overlap
**Options Considered**:
1. 500 chars / 100 overlap (smaller, more precise)
2. 1000 chars / 200 overlap (moderate)
3. 2000 chars / 400 overlap (larger, more context)

**Decision**: Option 2 - 1000 / 200

**Rationale**:
- Constitution specifies 1000 characters
- Balances precision vs context
- Fits well within Gemini context window
- 200 overlap ensures continuity across boundaries
- Cohere embedding model handles this size well

**Trade-offs**:
- May split important context across chunks
- Larger chunks = fewer total chunks = less granular search
- Can be tuned later if retrieval quality suffers

#### Decision 6: Top 5 Results
**Options Considered**:
1. Top 3 (minimal context)
2. Top 5 (moderate context)
3. Top 10 (maximum context)

**Decision**: Option 2 - Top 5

**Rationale**:
- Constitution specifies top 5
- Balances context richness vs noise
- Fits comfortably in Gemini prompt
- Reduces irrelevant information
- 5 chunks = ~5000 chars = manageable for LLM

**Trade-offs**:
- May miss relevant information beyond top 5
- More results = higher latency (negligible with Qdrant)
- Can be made configurable later

### 9. Performance Optimization

#### 9.1 Chat Endpoint (<3s target)
**Strategies**:
- Async I/O throughout (asyncio, httpx)
- Parallel operations where possible:
  - Embed query + warm Qdrant connection concurrently
- Connection pooling for Qdrant
- Cohere SDK async client
- OpenAI SDK async client
- Timeout on Qdrant search (5s max)

**Estimated Latency Breakdown**:
- Query embedding (Cohere): ~200ms
- Qdrant search: ~100ms
- Gemini generation: ~1-2s
- **Total**: ~1.5-2.5s (well under 3s target)

#### 9.2 Ingestion Endpoint (<5min target)
**Strategies**:
- Batch embeddings (96 at a time per Cohere limit)
- Async fetching of pages (httpx.AsyncClient)
- Parallel processing of independent pages
- Bulk upsert to Qdrant (batch inserts)
- Progress logging for monitoring

**Estimated Duration** (50 pages):
- Fetch all pages: ~30s (parallel)
- Chunk 50 pages: ~5s
- Embed ~1000 chunks in batches: ~90s
- Upsert to Qdrant: ~30s
- **Total**: ~2.5-3 minutes (well under 5min target)

#### 9.3 Health Endpoint (<100ms target)
**Strategies**:
- Single Qdrant ping operation
- Fast collection exists check
- Cached connection (don't reconnect each time)
- Timeout after 100ms

**Estimated Latency**: ~30-50ms (well under 100ms target)

### 10. Security Considerations

**Environment Variables**:
- All API keys loaded from .env (never hardcoded)
- No secrets in logs (sanitize error messages)
- No secrets in git (`.env` in `.gitignore`)

**CORS**:
- Strict origin whitelist (only FRONTEND_URL)
- No wildcard origins
- Credentials allowed only for whitelisted origin

**Input Validation**:
- Pydantic models enforce types and constraints
- Query length capped at 500 chars
- Empty/whitespace-only queries rejected
- URL validation for sitemap

**Error Messages**:
- Never expose internal details (stack traces, API keys)
- Generic messages for external clients
- Detailed logging internally with request_id

**Rate Limiting**:
- Not implemented in MVP (per constitution)
- Future consideration: 100 req/min per IP

### 11. Monitoring and Observability

**Metrics to Track**:
- Request count per endpoint
- Response latency (p50, p95, p99)
- Error rate by type (4xx, 5xx)
- External API latency (Cohere, Gemini, Qdrant)
- Qdrant collection size
- Ingestion success/failure rate

**Logging Events**:
- Every request start/end with duration
- All errors with context
- External API calls with latency
- Ingestion progress (every 10 pages)

**Health Checks**:
- Liveness: GET /health (Qdrant connectivity)
- Readiness: Collection exists and has data
- Startup: Environment variables loaded

### 12. Implementation Phases

**Phase 0: Setup** (0.5 hours)
- Initialize project structure
- Install dependencies
- Configure .env and config.py
- Set up logger

**Phase 1: Core Services** (3 hours)
- Implement embedding service (Cohere)
- Implement vector store service (Qdrant)
- Implement chunking utility
- Unit tests for services

**Phase 2: Agent Service** (2 hours)
- Implement Gemini agent with OpenAI SDK
- Configure retrieve() tool
- Test tool calling
- Unit tests for agent

**Phase 3: Ingestion Pipeline** (2 hours)
- Implement ingestion service
- Sitemap fetching
- Text extraction with Trafilatura
- Integration with chunking, embedding, vector store
- Unit tests for ingestion

**Phase 4: API Endpoints** (2 hours)
- Implement POST /chat
- Implement POST /ingest
- Implement GET /health
- Request/response models
- Integration tests for endpoints

**Phase 5: Middleware & Error Handling** (1 hour)
- CORS middleware
- Request ID middleware
- Exception handlers
- Structured logging

**Phase 6: Testing** (2 hours)
- Complete test coverage (>80%)
- Edge case tests
- Performance tests
- Chaos testing (simulate API failures)

**Phase 7: Deployment** (1 hour)
- Create vercel.json
- Set up environment variables in Vercel
- Deploy to Vercel
- Test deployed endpoints

**Phase 8: Validation** (1 hour)
- Run ingestion on production
- Test chat with sample queries
- Verify performance targets
- Monitor logs

**Total Estimated Duration**: 14.5 hours

### 13. Risk Mitigation

**Risk 1: Free Tier Rate Limits**
- **Mitigation**: Exponential backoff on all external API calls
- **Monitoring**: Log API usage per request
- **Fallback**: Return 503 with retry-after header

**Risk 2: Slow Ingestion**
- **Mitigation**: Parallel processing, batch embeddings
- **Monitoring**: Log progress every 10 pages
- **Fallback**: Increase batch size if under free tier limit

**Risk 3: Hallucination**
- **Mitigation**: Strong system prompt, cite sources, validate sources in response
- **Monitoring**: Manual validation on sample queries (SC-004, SC-005)
- **Fallback**: Add post-processing to validate sources exist

**Risk 4: CORS Issues**
- **Mitigation**: Test from actual Vercel domain during deployment
- **Monitoring**: Log CORS errors
- **Fallback**: Add OPTIONS preflight handler if needed

**Risk 5: Qdrant Timeout**
- **Mitigation**: Set 5s timeout on Qdrant searches
- **Monitoring**: Log slow queries (>2s)
- **Fallback**: Return 504 with clear error message

## Next Steps

1. **Review this plan** with stakeholder for approval
2. **Run `/sp.tasks`** to generate implementation tasks from this plan
3. **Begin Phase 0** (setup) and work sequentially through phases
4. **Create ADR** if any architectural decisions change significantly

## Open Questions

None - all design decisions resolved in this plan.
